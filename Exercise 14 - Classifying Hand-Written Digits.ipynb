{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we will use both our own neural network algorithm, and Tensorflow library. To make it easier for people using Google Colab to follow, we will load the MNIST data set from the TensorFlow library that comes preinstalled with colab. Users loading these notebooks in their own environments will have to install Tensorflow anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the TensorFlow library as `tf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the MNIST dataset as `mnist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function `load_data`, we load the MNIST dataset into the variable `dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further unpack our dataset into 4 variables containing training and validation data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (validation_data, validation_labels) = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length/shape of our training and validation data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset does not have a testing set, create one from the training set. Take the last 10000 data points from it and put it into variables `test_data` and `test_labels`. Then remove these points from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the first training data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function visualises the matrix of numbers as an image. Visualise some other data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[50], cmap=cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selected training data point, print the corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the labels of the first 100 training data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the values of 14th row of the first training data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the values of the 8th column of the first data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the values in the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum value in our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the values of all data sets between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the values of 14th row of the first training data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the values in the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the first data point in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape this data point into a column vector and store it in a variable `dp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of the vector stored in the variable `dp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape all the data as column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our own Neural Network algorithm for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the labels of the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose we can reuse the function `convert_label` that we defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(x):\n",
    "    vec = np.zeros((10,1))\n",
    "    vec[x] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the label of the first data point in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the label sets into numpy arrays: `train_labels_new`,  `validation_labels_new` and `test_labels_new`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of our new training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes in the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart for two elements, the algorithm we will use here is identical to the algorithms we have been using so far. What differs is:\n",
    "- When initialising weights, we divide each weight by the square root of the number of activations in the previous layer. This makes weights values smaller for the layer after the input one. This is done because the input layer has 784 activations which would make weights in the following layer too large.\n",
    "- We are using mini-batch training. Online training would be too slow, and batch training is not feasible since we have 50.000 data points. To learn step-by-step how we made mini batches, check the notebook named 'Appendix 01 - Mini batch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use mini batches, we will zip together data with labels into a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(list(zip(train_data, train_labels_new)))\n",
    "validation_set = np.array(list(zip(validation_data, validation_labels_new)))\n",
    "testing_set = np.array(list(zip(test_data, test_labels_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def evaluate_accuracy(dset, round_digits):\n",
    "    num_correct = 0;\n",
    "    for a, y in dset:\n",
    "        for W, b in zip(weights, biases):\n",
    "            a = sigmoid(np.dot(W, a) + b)\n",
    "        if (np.argmax(a)==np.argmax(y)):\n",
    "            num_correct += 1\n",
    "    return np.round(100*(num_correct / len(dset)),round_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [784, 100, 10]\n",
    "\n",
    "num_epochs = 8\n",
    "step_size = 3\n",
    "biases = [np.random.randn(a,1) for a in sizes[1:]]\n",
    "weights = [np.random.randn(nout, nin)/np.sqrt(nin) for nout, nin in zip(sizes[1:], sizes[:-1])]\n",
    "d_biases = [np.zeros(b.shape) for b in biases]\n",
    "d_weights = [np.zeros(w.shape) for w in weights]\n",
    "no_layers = len(sizes)\n",
    "datalen = len(train_set)\n",
    "mini_batch_length = 10\n",
    "\n",
    "for epoch in np.arange(num_epochs):\n",
    "    TC = 0\n",
    "    np.random.shuffle(train_set)\n",
    "    mini_batches = [train_set[a:a+mini_batch_length] \n",
    "                    for a in range(0, datalen, mini_batch_length)]\n",
    "    for batch in mini_batches:\n",
    "        dd_biases = [np.zeros(b.shape) for b in biases]\n",
    "        dd_weights = [np.zeros(w.shape) for w in weights]\n",
    "        # single point (a,y)\n",
    "        for a, y in batch:\n",
    "            activations = [a]\n",
    "            weighted_sums = []\n",
    "            for W, b in zip(weights, biases):\n",
    "                z = np.dot(W, a) + b\n",
    "                weighted_sums.append(z)\n",
    "                a = sigmoid(z)\n",
    "                activations.append(a)\n",
    "            # cost\n",
    "            C = np.sum((a-y)**2)\n",
    "            TC += C\n",
    "            # backward pass\n",
    "            dC = 2*(a-y)\n",
    "            delta = dC * a * (1 - a)\n",
    "            d_biases[-1] = delta\n",
    "            d_weights[-1] = np.dot(delta, activations[-2].T)\n",
    "            for i in range(2, no_layers):\n",
    "                delta = activations[-i]*(1-activations[-i])*np.dot(weights[-i+1].T,delta)\n",
    "                d_biases[-i] = delta\n",
    "                d_weights[-i] = np.dot(delta, activations[-i-1].T)\n",
    "            dd_weights = [dw+ddw for dw, ddw in zip(d_weights, dd_weights)]\n",
    "            dd_biases = [db+ddb for db, ddb in zip(d_biases, dd_biases)]\n",
    "        blen = len(batch)\n",
    "        weights = [d-dw/blen*step_size for d, dw in zip(weights, dd_weights)]\n",
    "        biases = [d-db/blen*step_size for d, db in zip(biases, dd_biases)]\n",
    "    acc_train = evaluate_accuracy(train_set,4)\n",
    "    acc_validation = evaluate_accuracy(validation_set,4)\n",
    "    print (f'epoch: {epoch+1} | total cost: {np.round(TC,4)}')\n",
    "    print (f\"Training set prediction accuracy:   {acc_train}%\")\n",
    "    print (f\"Validation set prediction accuracy: {acc_validation}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the accuracy of the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = evaluate_accuracy(testing_set, 4)\n",
    "print (f\"Testing set prediction accuracy: {acc_test}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for dp, y in testing_set:\n",
    "    a = dp\n",
    "    for W, b in zip(weights, biases):\n",
    "        a = sigmoid(np.dot(W, a) + b)\n",
    "    prediction = np.argmax(a)\n",
    "    label = np.argmax(y)\n",
    "    if not (prediction==label):\n",
    "        counter += 1\n",
    "        print (f'prediction: {prediction}')\n",
    "        print (f'label: {label}')\n",
    "        plt.imshow(dp.reshape(28,28), cmap=cm.Greys)\n",
    "        plt.show()\n",
    "    if counter==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow Keras for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating our own model in Tensorflow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(784,1)),\n",
    "  tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(10, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate = 3.0),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, \n",
    "                    train_labels_new, \n",
    "                    validation_data=(validation_data,validation_labels_new), \n",
    "                    epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels_new, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the recommended Tensorflow Keras parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(784,1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, \n",
    "                    train_labels, \n",
    "                    validation_data=(validation_data, validation_labels),\n",
    "                    epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - Tensorflow 2.0",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
